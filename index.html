<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Weihua Chen, Victor Chen, 陈威华, CRIPAC, NLPR, CASIA, BJTU, Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing Jiaotong University" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="shortcut icon" href="fig/cripac.png">
<title>Weihua Chen's Homepage</title>
</head>
<body>
<div id="layout-content">

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<table class="imgtable"><tr><td>
<img src="fig/zy.jpg" alt="alt text" width="130px" height="100px" /> &nbsp;</td>
<td align="right">

<div id="toptitle"> 
  <h1>
  <a href="https://gumpest.github.io/">Yuan Zhang</a> &nbsp; 张袁
  </h1>
</div>

<p>
Research Intern in Alibaba DAMO Academy.<br /><br />
Email: <a href="mailto:zhangyuan@stu.pku.edu.cn">zhangyuan at stu.pku.edu.cn</a><br /><br />
[<a href= "https://scholar.google.com.hk/citations?user=dXj1WskAAAAJ&hl=zh-CN">Google Scholar</a>]<br />
</p>
</td></tr></table>

<!-- <h2>News</h2>
<ul>
<li>
  <p>[Paper Accepted] One paper is accepted by <b>ECCV 2022</b>, and the code has been released [<a href="https://github.com/dcp15/UAL/tree/master">link</a>].</p>
</li>
<li>
  <p>[Paper Accepted] TagPerson is accepted by <b>ACM MM 2022</b>, and the code has been released [<a href="https://github.com/tagperson/tagperson-blender">link</a>].</p>
</li>
<li>
  <p>[Paper Accepted] One paper is accepted by <b>IEEE TIFS</b>.</p>
</li>
<li>
  <p>[Paper Accepted] CDTrans is accepted by <b>ICLR 2022</b>, and the code has been released [<a href="https://github.com/CDTrans/CDTrans">link</a>].</p>
</li>
</ul> -->

<h2>Biography</h2>

<p>
    Yuan Zhang is currently a 3th-year master candidate in <a href="https://english.pku.edu.cn/">Peking University</a>, 
    under the supervision of <a href="https://www.ss.pku.edu.cn/teacherteam/teacherlist/1657-%E6%9B%B9%E5%81%A5.html">Prof. Jian Cao</a>. <br />
    He received the B.S. degree from <a href="https://en.hhu.edu.cn/">Hohai University</a> in 2020 
    and won the National Scholarship with ranking First in major.
</p>
    
<p> He major research interests lie within computer vision and model compression, such as </p>
    <ul>
        <li style="margin-top: 0.2em">Light Weight Object Detection</li>
        <li style="margin-top: 0.2em">Knowledge distillation (KD)</li>
    </ul>

<h2>Publications</h2> 
<ul>

<li>
<a href="https://arxiv.org/pdf/2205.14589.pdf">Masked Distillation with Receptive Tokens</a> <br />
Tao Huang*, <b>Yuan Zhang</b>*, Shan You, Fei Wang, Chen Qian, Jian Cao, Chang Xu<br />
<i>The International Conference on Learning Representations</i> (<b>ICLR</b>), 2023. <br />
</li>
[<a href="https://arxiv.org/pdf/2205.14589.pdf">PDF</a>]
[<a href="https://github.com/hunto/MasKD">Code</a>]
<br /><br />

<li>
<a href="https://arxiv.org/abs/2110.01240">A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition (AFTrans)</a> <br />
<b>Yuan Zhang</b>, Jian Cao, Ling Zhang, Xiangcheng Liu, Feng Ling, Weiqian Chen<br />
<i>IEEE International Conference on Acoustics, Speech and Signal Processing</i> (<b>ICASSP</b>), 2022. <br />
</li>
[<a href="https://arxiv.org/pdf/2110.01240.pdf">PDF</a>]
<br /><br />
  
<li>
<a href="https://arxiv.org/pdf/2211.15444.pdf">DAMO-YOLO: A Report on Real-Time Object Detection Design</a> <br />
Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang, <b>Yuan Zhang</b>, Xiuyu Sun <br />
<i>arXiv preprint arXiv:2211.15444</i>, 2022. <br />
</li>
[<a href="https://arxiv.org/pdf/2211.15444.pdf">PDF</a>]
<br /><br />
  
<li>
<a href="https://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sun_An_Once-for-All_Budgeted_Pruning_Framework_for_ConvNets_Considering_Input_Resolution_CVPRW_2022_paper.pdf"> 
  An Once-for-All Budgeted Pruning Framework for ConvNets Considering Input Resolution</a> <br />
Wenyu Sun, Jian Cao, Pengtao Xu, Xiangcheng Liu, <b>Yuan Zhang</b>, Yuan Wang <br />
<i>The Conference on Computer Vision and Pattern Recognition</i> (<b>CVPR Workshop</b>), 2022. <br />
</li>
[<a href="https://openaccess.thecvf.com/content/CVPR2022W/ECV/papers/Sun_An_Once-for-All_Budgeted_Pruning_Framework_for_ConvNets_Considering_Input_Resolution_CVPRW_2022_paper.pdf">PDF</a>]
<br /><br />


</ul>

<h2>Activity</h2>
<ul>
  
  
<li>
Invited tutorial talk in IJCB 2021 with the topic of <a href="https://ijcb2021.iapr-tc4.org/tutorials/#Tutorial_3_Human-centric_Visual_Understanding_From_Research_to_Applications">Human-centric Visual Understanding: From Research to Applications</a>.
</li><br />
  
<li>
Win the 2nd (2/263, top1%) place in Google Landmark Retrieval Competition on ICCV 2021 and silver medal in Google Landmark Recognition Competition.
[<a href="https://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution/blob/master/ILR2021_2nd_solution.pdf">PDF</a>]
[<a href="https://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution">Code</a>]
[<a href="https://www.kaggle.com/c/landmark-retrieval-2021/discussion/277273">Kaggle Poster</a>]
[<a href="https://github.com/WesleyZhang1991/Google_Landmark_Retrieval_2021_2nd_Place_Solution/blob/master/ILR21_RET_2nd-slides.pdf">Slides</a>]
[<a href="https://www.youtube.com/watch?v=bkT2Judxf_s">Video</a>] 
</li><br />

<li>
Win the 1st place in <a href="https://iccv2021-mmp.github.io/">Multi-camera Multi-Person tracking</a> on ICCV 2021.
</li><br />

<li>
Win the 1st place in <a href="https://www.aicitychallenge.org/">AICITY Challenge</a> Track3 Multi-camera Vehicle Tracking on CVPR 2021.
[<a href="https://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Liu_City-Scale_Multi-Camera_Vehicle_Tracking_Guided_by_Crossroad_Zones_CVPRW_2021_paper.pdf">PDF</a>]
[<a href="https://github.com/LCFractal/AIC21-MTMC">Code</a>] 
</li><br />
  
<li>
Win the 1st place in <a href="https://www.aicitychallenge.org/">AICITY Challenge</a> Track2 Vehicle Re-Identification on CVPR 2021.
[<a href="https://openaccess.thecvf.com/content/CVPR2021W/AICity/papers/Luo_An_Empirical_Study_of_Vehicle_Re-Identification_on_the_AI_City_CVPRW_2021_paper.pdf">PDF</a>]
[<a href="https://github.com/michuanhaohao/AICITY2021_Track2_DMT">Code</a>] 
</li><br />
  
<li>
Win the 2nd place in <a href="https://github.com/JonathonLuiten/TrackEval/blob/master/docs/RobMOTS-Official/Readme.md">RobMOTS: The Ultimate Tracking Challenge</a> on CVPR 2021.
[<a href="https://omnomnom.vision.rwth-aachen.de/data/RobMOTS/workshop/challenge/2nd/SBT_RobMOTS.pdf">PDF</a>]
</li><br />
  
<li>
Win the 1st place <a href="https://motchallenge.net/results/TAO_Challenge/">Tracking Any Objects (TAO) Challenge</a> on ECCV 2020.
[<a href="https://arxiv.org/abs/2101.08040">PDF</a>]
[<a href="https://github.com/feiaxyt/Winner_ECCV20_TAO">Code</a>] 
</li><br />
  
<li>
Win the 1st place <a href="http://ai.bu.edu/visda-2020/">Visual Domain Adaptation (VisDA) Challenge</a> on ECCV 2020.
[<a href="https://arxiv.org/abs/2012.13498">PDF</a>]
[<a href="https://github.com/vimar-gu/Bias-Eliminate-DA-ReID">Code</a>] 
</li><br />
    
<li>
Win the 3rd place in <a href="https://www.aicitychallenge.org/2020-ai-city-challenge/">AICITY Challenge</a> Track2 Vehicle Re-Identification on CVPR 2020.
[<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w35/He_Multi-Domain_Learning_and_Identity_Mining_for_Vehicle_Re-Identification_CVPRW_2020_paper.pdf">PDF</a>]
[<a href="https://github.com/heshuting555/AICITY2020_DMT_VehicleReID">Code</a>] 
</li><br />
  
<li>
Organize <a href="http://mct.idealtest.org/">the Multi-Camera Object Tracking (<b>MCT</b>) Challenge </a>
in <a href="http://www.vs-re-id-2014.org/">Visual Surveillance and Re-identification Workshop</a> on ECCV 2014
</li><br />
  
<li>
Serve as Reveiwer for top conferences and journals, such as PAMI/CVPR/ICCV/ECCV.
</li><br />
</ul>

<p>
  <a href="http://english.ia.cas.cn/"><img src="fig/casia.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.nlpr.ia.ac.cn/"><img src="fig/nlpr.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.cripac.ia.ac.cn/"><img src="fig/cripac.png" alt="alt text" width="50px" height="50px"/></a> 

  <a href="http://www.bjtu.edu.cn/"><img src="fig/bjtu.jpg" alt="alt text" width="50px" height="50px"/></a> 
</p>

<div id="footer">
<div id="footer-text">
</br>Last updated at 2022-07-01 by Weihua Chen.
</div>
</div>
</div>


</body>
</html>
